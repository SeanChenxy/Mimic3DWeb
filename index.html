<!DOCTYPE html>

<html>

<head lang="en">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Mimic3D</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="./files/bootstrap.min.css">
    <link rel="stylesheet" href="./files/font-awesome.min.css">
    <link rel="stylesheet" href="./files/codemirror.min.css">
    <link rel="stylesheet" href="./files/app.css">




</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-20 text-center">
                <br></br>
                <b>Mimic3D</b>: Thriving 3D-Aware GANs via 3D-to-2D Imitation<br>
                <small>
                    ICCV 2023
                </small>
            </h1>
            <hr style="margin-top:0px">
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://seanchenxy.github.io/" style="font-size: 16px;">
                            Xingyu Chen
                        </a>
                        <sup>*</sup>
                    </li>
                    <li>
                        <a href="https://yudeng.github.io/" style="font-size: 16px;">
                            Deng Yu
                        </a>
                        <sup>*</sup>
                    </li>
                    <li>
                        <a href="https://sites.google.com/site/zjuwby/" style="font-size: 16px;">
                            Baoyuan Wang
                        </a>
                        <!-- <sup>3</sup> -->
                    </li>
                    <br>
                    <a></a><br>
                    <li>
                        <!-- <sup>1</sup> -->
                        <a href="https://www.xiaoice.com/" style="font-size: 16px;">
                            Xiaobing.AI
                        </a>
                    </li>
                    <li>
                        <sup>*</sup>
                        These authors contributed equally to this work
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2303.09036">
                            <img src="./asserts/arxiv.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>

                    <li>
                        <a onClick="alert('Code coming soon!\nContact chenxingyu@xiaobing.ai for more details.')">
                        <!-- <a href=""> -->
                            <img src="./asserts/github.png" height="60px">
                            <h4><strong>Code (coming soon)</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <a>
                    <!-- <img style="width:100%;height:100%;" playsinline autoplay loop preload muted>
                        <source src="./asserts/teaser.png">
                    </img> -->
                    <img  style="width:100%;height:100%;" src="./asserts/teaser.png">
                </a>
                <p class="text-justify" style="font-size: 16px;">
                    Our method enables high-quality image generation at 512 × 512 resolution without using a 2D super-resolution module.
                </p>
                <br></br>
                <h2>
                    Abstract
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    Generating images with both photorealism and multiview 3D consistency is crucial for 3D-aware GANs, yet existing methods struggle to achieve them simultaneously. Improving the photorealism via CNN-based 2D super-resolution can break the strict 3D consistency, while keeping the 3D consistency by learning high-resolution 3D representations for direct rendering often compromises image quality. In this paper, we propose a novel learning strategy, namely 3D-to-2D imitation, which enables a 3D-aware GAN to generate high-quality images while maintaining their strict 3D consistency, by letting the images synthesized by the generator's 3D rendering branch to mimic those generated by its 2D super-resolution branch. We also introduce 3D-aware convolutions into the generator for better 3D representation learning, which further improves the image generation quality. With the above strategies, our method reaches FID scores of 5.4 and 4.3 on FFHQ and AFHQ-v2 Cats, respectively, at 512$\times$512 resolution, largely outperforming existing 3D-aware GANs using direct 3D rendering and coming very close to the previous state-of-the-art method that leverages 2D super-resolution. 
                </p>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Video
                </h2>
                <hr style="margin-top:0px">
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/hBJWZwl_JCI" allowfullscreen=""
                            style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Overview
                </h2>
                <hr style="margin-top:0px">
                <img src="./asserts/framework.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify" style="font-size: 16px;">
                    Overview of our framework. 3D-to-2D imitation strategy is enforced to let the generator's 3D branch to mimic the results of its 2D branch, thus leading to image generation of high quality and strict 3D consistency. 3D-aware convolutions are also introduced to the tri-plane generator to enhance 3D representation learning, which further improves the image generation quality.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    3D-aware tri-plane generator
                </h2>
                <hr style="margin-top:0px">
                <img src="./asserts/3daware.png" class="img-responsive" alt="illum"><br>
                <p class="text-justify" style="font-size: 16px;">
                    (a) Structure of our 3D-aware tri-plane generator. (b) Operations of the 3D-aware block on xy plane.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Comparisons with Prior Arts
                </h2>
                <hr style="margin-top:0px">
                <!-- <div align="center">
                    <img src="./asserts/compare.png" class="img-responsive" alt="compare"><br>
                </div> -->
                <img src="./asserts/compare.png" class="img-responsive" alt="illum"><br>
                <p class="text-justify" style="font-size: 16px; text-align: center">
                    When compared to EG3D on FFHQ at 512 resolution, our method generates images with comparable quality to those of EG3D, while producing 3D geometries with finer details and multiview sequences with better 3D-consistency.
                    When compared to other 3D rendering baselines on FFHQ at 512 resolution, our method generates images with higher quality.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Dynamic Demonstrations
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    Comparison with EG3D on 512-Resolution Face Generation. We have on-par image quality and superior multi-view consistency.
                </p>
                <video style="width:100%;height:100%;" playsinline autoplay loop preload muted>
                    <source src="./asserts/eg3d_comp.mp4" type="video/mp4">
                </video>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    Comparison with EpiGRAF and GMPI on 512-Resolution Face Generation. We have superior image quality.
                </p>
                <video style="width:100%;height:100%;" playsinline autoplay loop preload muted>
                    <source src="./asserts/3d_comp.mp4" type="video/mp4">
                </video>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    More our results on FFHQ.
                </p>
                <video style="width:100%;height:100%;" playsinline autoplay loop preload muted>
                    <source src="./asserts/ours_face.mp4" type="video/mp4">
                </video>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    More our results on AFHQ-v2 Cats.
                </p>
                <video style="width:100%;height:100%;" playsinline autoplay loop preload muted>
                    <source src="./asserts/ours_cat.mp4" type="video/mp4">
                </video>
            </div>
        </div>



        <!-- <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Manifolds Visualization
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    GRAM constrains point sampling and radiance field learning on 2D manifolds, embodied as a set of
                    implicit surfaces. These implicit surfaces are shared for the trained object category, jointly
                    learned with GAN training, and fixed at inference time.
                </p>
                <video style="width:93%;height:93%;" playsinline autoplay loop preload muted>
                    <source src="./files/manifold.mp4" type="video/mp4">
                </video>
            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    3D Geometry Visualization
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    Although GRAM confines the input domain of the radiance field on 2D manifolds, we can still extract
                    proxy 3D shapes of the generated objects using the volume-based marching cubes algorithm. It can be
                    observed that GRAM produces high-quality geometry with detailed structures well depicted, which is
                    the key to achieve strong visual
                    3D consistency across different views.
                </p>
                <video style="width:100%;height:100%;" playsinline autoplay loop preload muted>
                    <source src="./files/shape.mp4" type="video/mp4">
                </video>
            </div>
        </div> -->

<!--         <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Image Embedding and Editing
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                GAN inversion is naturally supported by GRAM. Given an input image, we can first embed it into the learned latent space and then freely move the camera viewpoint to synthesize images at novel views.
                </p>
                <video style="width:67%;height:67%;" playsinline autoplay loop preload muted>
                    <source src="./files/inv2.mp4" type="video/mp4">
                </video>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Responsible AI Considerations
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    The goal of this paper is to study generative modelling of the 3D objects from 2D images, and to provide a method for generating multi-view images of non-existing, virtual objects. 
                    It is not intended to manipulate existing images nor to create content that is used to mislead or deceive. 
                    This method does not have understanding and control of the generated content. 
                    Thus, adding targeted facial expressions or mouth movements is out of the scope of this work. 
                    However, the method, like all other related AI image generation techniques, could still potentially be misused for impersonating humans. 
                    Currently, the images generated by this method contain visual artifacts, unnatural texture patterns, and other unpredictable failures that can be spotted by humans and fake image detection algorithms. 
                    We also plan to investigate applying this technology for advancing 3D- and video-based forgery detection.
                </p>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Availability of Software
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    Per concerns about misuse of this method, the code is available for use under a research-only license.
                </p>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-12 col-md-offset-0">
                <div class="text-center">
                    <h2>
                        Citation
                    </h2>
                </div>
                <hr style="margin-top:0px">
                <div class="form-group col-md-12 col-md-offset-0">
                    <div class="CodeMirror cm-s-default CodeMirror-wrap" style="font-size: 16px;">
                        <div
                            style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 4px; left: 4px; ">
                            <textarea autocorrect="off" autocapitalize="off" spellcheck="false"
                                style="position: absolute; padding: 0px; width: 1000px; height: 1em; outline: none;"
                                tabindex="0"></textarea></div>
                        <div class="CodeMirror-vscrollbar" cm-not-content="true">
                            <div style="min-width: 1px; height: 0px;"></div>
                        </div>
                        <div class="CodeMirror-hscrollbar" cm-not-content="true">
                            <div style="height: 100%; min-height: 1px; width: 0px;"></div>
                        </div>
                        <div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div>
                        <div class="CodeMirror-gutter-filler" cm-not-content="true"></div>
                        <div class="CodeMirror-scroll" tabindex="-1">
                            <div class="CodeMirror-sizer"
                                style="margin-left: 0px; margin-bottom: -17px; border-right-width: 13px; min-height: 162px; padding-right: 0px; padding-bottom: 0px;">
                                <div style="position: relative; top: 0px;">
                                    <div class="CodeMirror-lines">
                                        <div style="position: relative; outline: none;">
                                            <div class="CodeMirror-measure">AخA</div>
                                            <div class="CodeMirror-measure"></div>
                                            <div style="position: relative; z-index: 1;"></div>
                                            <div class="CodeMirror-cursors">
                                                <div class="CodeMirror-cursor"
                                                    style="left: 4px; top: 0px; height: 17.1406px;">&nbsp;</div>
                                            </div>
                                            <div class="CodeMirror-code" style="">
                                                <pre
                                                    class=" CodeMirror-line "><span style="padding-right: 0.1px;">@inproceedings{bib:mimic3d,</span></pre>
                                                <pre
                                                    class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  title={Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation},</span></pre>
                                                <pre
                                                    class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  author={Chen, Xingyu and Deng, Yu and Wang, Baoyuan},</span></pre>
                                                <pre
                                                    class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},</span></pre>
                                                <pre
                                                    class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  year={2023}</span></pre>
                                                <pre
                                                    class=" CodeMirror-line "><span style="padding-right: 0.1px;">}</span></pre>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div style="position: absolute; height: 13px; width: 1px; top: 280px;"></div>
                            <div class="CodeMirror-gutters" style="display: none; height: 300px;"></div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Acknowledgements
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    The website template was adapted from <a href="https://yudeng.github.io/GRAM/">GRAM</a>.
                </p>
            </div>
        </div>


</body>

</html>
